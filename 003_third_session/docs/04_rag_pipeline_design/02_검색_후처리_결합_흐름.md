# 02. 검색-후처리-결합 흐름

## 이 챕터에서 배우는 것

- 03_rag_postprocessing 기반의 결합 흐름
- 정규화/후처리의 정확한 적용 순서
- 서비스 수준 체크포인트

---

## 1. 표준 흐름(권장 순서)

1) 다중 검색 수행
2) **정책 기반 사전 필터**
3) 점수 정규화(거리 → 유사도)
4) 결과 병합
5) 1차 후처리(중복 제거/다양성 확보)
6) 2차 후처리(재정렬/압축)

---

## 2. 그래프 노드 설계(검색~후처리 구간)

검색~후처리 구간은 다음 노드로 구성됩니다.

- `retrieve`: 다중 검색 수행
- `policy_filter`: 권한/언어/버전 필터
- `normalize`: 점수 정규화
- `merge`: 결과 병합
- `postprocess`: 중복 제거/다양성/재정렬/압축

> 이 구간은 **생성 단계 전**까지의 품질을 결정합니다.

---

## 3. 입력 표준화

다중 검색을 결합하려면 결과 스키마를 먼저 통일합니다.

```python
"""
목적: 검색 결과 표준 스키마를 정의한다.
설명: score_type으로 거리/유사도를 구분한다.
디자인 패턴: Value Object
"""

from dataclasses import dataclass


@dataclass(frozen=True)
class Candidate:
    """검색 후보 문서."""

    doc_id: str
    content: str
    metadata: dict
    score: float
    score_type: str  # "distance" 또는 "similarity"
    source_id: str
```

---

## 4. 정책 기반 사전 필터

정책/권한/언어/버전 필터를 **정규화 이전**에 적용합니다.

```python
"""
목적: 정책 기반 사전 필터를 적용한다.
설명: 권한/언어/버전 필터를 우선 적용한다.
디자인 패턴: Policy
"""

def apply_policy_filters(candidates: list[Candidate]) -> list[Candidate]:
    """정책 필터를 적용한다."""
    return [c for c in candidates if c.metadata.get("access_level") == "public"]
```

---

## 5. 점수 정규화

거리 점수를 유사도 점수로 통일한 뒤 병합합니다.

```python
"""
목적: 거리 점수를 유사도 점수로 변환한다.
설명: 1 / (1 + distance) 방식의 단조 변환을 사용한다.
디자인 패턴: Strategy
"""

def distance_to_similarity(distance: float) -> float:
    """거리 점수를 유사도 점수로 변환한다."""
    return 1.0 / (1.0 + distance)
```

---

## 6. 후처리 1: 중복 제거/다양성 확보

```python
"""
목적: 중복 제거와 다양성 확보를 적용한다.
설명: source_id 기준으로 중복 제거 후 출처 다양성을 제한한다.
디자인 패턴: Pipeline
"""

def dedupe_by_source(candidates: list[Candidate]) -> list[Candidate]:
    """source_id 기준 중복 제거."""
    seen: set[str] = set()
    result = []
    for c in candidates:
        if c.source_id in seen:
            continue
        seen.add(c.source_id)
        result.append(c)
    return result


def diversify_by_source(candidates: list[Candidate], max_per_source: int = 2) -> list[Candidate]:
    """출처 다양성을 확보한다."""
    counts: dict[str, int] = {}
    result = []
    for c in candidates:
        counts[c.source_id] = counts.get(c.source_id, 0)
        if counts[c.source_id] >= max_per_source:
            continue
        counts[c.source_id] += 1
        result.append(c)
    return result
```

---

## 7. 병합 + 후처리 2: 재정렬/압축

```python
"""
목적: 병합 후 재정렬과 컨텍스트 압축을 적용한다.
설명: 재정렬 후 상위 K개만 유지한다.
디자인 패턴: Pipeline
"""

def rerank(candidates: list[Candidate], top_k: int = 5) -> list[Candidate]:
    """점수 기준으로 재정렬한다."""
    return sorted(candidates, key=lambda x: x.score, reverse=True)[:top_k]


def compress_context(candidates: list[Candidate], limit: int = 5) -> list[Candidate]:
    """컨텍스트 길이를 제한한다."""
    return candidates[:limit]
```

---

## 8. 통합 파이프라인 예시(개념)

```python
"""
목적: 검색-후처리-결합 흐름을 통합한다.
설명: 정책 필터 → 정규화 → 병합 → 후처리 순서로 적용한다.
디자인 패턴: Pipeline
"""

def merge_pipeline(groups: list[list[Candidate]]) -> list[Candidate]:
    """전략별 결과를 병합한다."""
    filtered_groups = [apply_policy_filters(g) for g in groups]
    normalized_groups: list[list[Candidate]] = []
    for group in filtered_groups:
        normalized_group: list[Candidate] = []
        for c in group:
            if c.score_type == "distance":
                normalized_group.append(
                    Candidate(
                        doc_id=c.doc_id,
                        content=c.content,
                        metadata=c.metadata,
                        score=distance_to_similarity(c.score),
                        score_type="similarity",
                        source_id=c.source_id,
                    )
                )
            else:
                normalized_group.append(c)
        normalized_groups.append(normalized_group)

    merged = [c for g in normalized_groups for c in g]
    merged = dedupe_by_source(merged)
    merged = diversify_by_source(merged)
    merged = rerank(merged, top_k=5)
    return compress_context(merged, limit=5)
```

---

## 9. 흔한 실수

- 정책 필터가 뒤로 밀려 **보안 위험** 발생
- 정규화 없이 병합해 **편향** 발생
- 중복 제거/다양성 확보 없이 **동일 출처 과다 노출**

---

## 10. 체크리스트

- 정책 필터가 먼저 적용되는가?
- `score_type` 기준 정규화가 수행되는가?
- 후처리 단계가 중복되지 않는가?
- 병합 후 Top-K 제한이 있는가?
- 컨텍스트 길이가 제한되는가?
