# 01. 벡터 검색 개요

## 이 챕터에서 배우는 것

- 벡터 검색이 실제로 수행되는 작업 흐름
- 구현에 필요한 핵심 클래스/함수의 역할
- 서비스 운영에서 필요한 주요 구성 요소
- 품질과 비용을 좌우하는 설계 포인트

---

## 용어/표기 기준

- **점수(score)**: 검색 결과에 부여되는 값의 총칭
- **거리 점수(distance score)**: 값이 **작을수록** 유사
- **유사도 점수(similarity score)**: 값이 **클수록** 유사
- **리트리버(retriever)**: 검색 인터페이스(질의 → 문서 목록)
- **벡터 스토어(vector store)**: 벡터 저장/검색 컴포넌트

---

## 1. 벡터 검색은 어떤 작업들의 조합인가?

벡터 검색은 단순히 “유사한 문서를 찾는다”가 아니라, 아래 작업들이 **연결된 파이프라인**입니다.

1) **문서 수집**: 원본 문서를 모아 정제한다
2) **청크 분할**: 문서를 검색 가능한 단위로 나눈다
3) **임베딩 생성**: 텍스트를 벡터로 변환한다
4) **저장**: 벡터와 메타데이터를 DB에 저장한다
5) **검색**: 쿼리 임베딩으로 유사 문서를 조회한다
6) **후처리**: 중복 제거/필터링/재정렬을 수행한다
7) **응답 조립**: 서비스에 맞는 형식으로 결과를 제공한다

> 벡터 검색은 **저장 → 검색 → 후처리** 3단계가 핵심 축입니다.

---

## 2. 구현에 필요한 핵심 클래스/함수(큰 요소 단위)

아래 구성 요소가 있어야 **서비스 수준 구현**이 가능합니다.

### 1) 문서 모델(Document Model)

- 문서 본문과 메타데이터를 하나로 표현
- 청크 단위 기준이 되는 기본 객체

```python
"""
목적: 문서 데이터를 일관된 형태로 표현한다.
설명: 청크/메타데이터와 함께 저장하기 위한 모델이다.
디자인 패턴: Value Object
"""

from dataclasses import dataclass


@dataclass(frozen=True)
class Document:
    """문서 데이터 모델."""

    content: str
    metadata: dict
```

### 2) 청크 분할기(Chunker)

- 문서를 검색 가능한 단위로 나눈다
- 청크 크기/오버랩/분할 기준을 관리한다

```python
"""
목적: 문서를 청크 단위로 분할한다.
설명: 문단 기준 분할 후 순서를 부여한다.
디자인 패턴: Factory
"""

from dataclasses import dataclass


@dataclass(frozen=True)
class Chunk:
    """청크 데이터 모델."""

    source_id: str
    chunk_id: str
    chunk_index: int
    content: str


def build_chunks(source_id: str, text: str) -> list[Chunk]:
    """문단 기준 청크를 생성한다."""
    paragraphs = [p.strip() for p in text.split("\n\n") if p.strip()]
    chunks: list[Chunk] = []
    for idx, paragraph in enumerate(paragraphs):
        chunks.append(
            Chunk(
                source_id=source_id,
                chunk_id=f"{source_id}:{idx}",
                chunk_index=idx,
                content=paragraph,
            )
        )
    return chunks
```

### 3) 임베딩 서비스(Embedding Service)

- 텍스트 → 벡터 변환을 담당
- 모델 선택, 배치 처리, 실패 재시도를 관리한다

```python
"""
목적: 텍스트를 임베딩 벡터로 변환한다.
설명: OpenAI 임베딩 API 호출 흐름을 단순화한다.
디자인 패턴: Service
"""

from typing import Sequence
import os
import requests


class EmbeddingService:
    """임베딩 생성 서비스."""

    def __init__(self, model: str) -> None:
        self._model = model

    def embed(self, texts: Sequence[str]) -> list[list[float]]:
        """여러 텍스트를 임베딩으로 변환한다."""
        url = "https://api.openai.com/v1/embeddings"
        headers = {
            "Authorization": f"Bearer {os.environ['OPENAI_API_KEY']}",
            "Content-Type": "application/json",
        }
        payload = {"model": self._model, "input": list(texts)}
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        response.raise_for_status()
        return [item["embedding"] for item in response.json()["data"]]
```

### 4) 저장소(Repository)

- 벡터/메타데이터를 DB에 저장하고 검색한다
- 검색 결과를 **문서 형태로 반환**하는 것이 중요하다

```python
"""
목적: 문서 저장/검색을 담당하는 저장소를 정의한다.
설명: 검색 결과를 문서 객체로 반환한다.
디자인 패턴: Repository
"""

from dataclasses import dataclass
from typing import Sequence
import psycopg


@dataclass(frozen=True)
class StoredDocument:
    """저장된 문서 조회 결과."""

    doc_id: int
    content: str
    metadata: dict
    score: float  # 거리 기반 점수(작을수록 유사)


class DocumentRepository:
    """문서 저장과 검색을 담당한다."""

    def __init__(self, dsn: str, table_name: str = "documents") -> None:
        self._dsn = dsn
        self._table_name = table_name

    def save(self, content: str, embedding: Sequence[float], metadata: dict) -> None:
        """문서를 저장한다."""
        sql = f"""
        INSERT INTO {self._table_name} (content, embedding, metadata)
        VALUES (%s, %s, %s)
        """
        with psycopg.connect(self._dsn) as conn:
            conn.execute(sql, (content, embedding, metadata))

    def search(self, query_vector: Sequence[float], top_k: int) -> list[StoredDocument]:
        """유사 문서를 검색한다."""
        sql = f"""
        SELECT id, content, metadata, (embedding <=> %s) AS score
        FROM {self._table_name}
        ORDER BY embedding <=> %s
        LIMIT %s
        """
        with psycopg.connect(self._dsn) as conn:
            rows = conn.execute(sql, (query_vector, query_vector, top_k)).fetchall()
        return [StoredDocument(*row) for row in rows]
```

> 위 예시의 `score`는 **거리(distance)** 이므로 **작을수록 더 유사**합니다.
> 테이블명은 고정 값만 허용해야 하며, 동적 구성 시에는 `psycopg.sql.Identifier`를 사용해야 합니다.

### 5) 검색 서비스(Search Service)

- 검색 + 후처리 + 결과 포맷을 담당
- 서비스 레벨 품질 기준을 적용한다

```python
"""
목적: 검색 후 결과를 정제해 반환한다.
설명: 점수 임계값과 중복 제거를 적용한다.
디자인 패턴: Facade
"""

from dataclasses import dataclass


@dataclass(frozen=True)
class SearchResult:
    """최종 검색 결과."""

    content: str
    metadata: dict
    score: float


def filter_by_distance(results: list[StoredDocument], max_distance: float) -> list[StoredDocument]:
    """거리 기반 임계값을 기준으로 필터링한다."""
    return [r for r in results if r.score <= max_distance]


def dedupe_by_source(results: list[StoredDocument]) -> list[StoredDocument]:
    """source_id 기준으로 중복을 제거한다."""
    seen: set[str] = set()
    deduped: list[StoredDocument] = []
    for r in results:
        source_id = r.metadata.get("source_id", "")
        if source_id and source_id in seen:
            continue
        seen.add(source_id)
        deduped.append(r)
    return deduped


class SearchService:
    """검색 결과를 후처리하고 반환한다."""

    def __init__(self, repo: DocumentRepository, max_distance: float) -> None:
        self._repo = repo
        self._max_distance = max_distance  # 예시값: 실제 서비스에서는 캘리브레이션 필요

    def search(self, query_vector: Sequence[float], top_k: int) -> list[SearchResult]:
        """검색 후 필터링/중복 제거를 적용한다."""
        raw = self._repo.search(query_vector, top_k)
        filtered = filter_by_distance(raw, max_distance=self._max_distance)
        deduped = dedupe_by_source(filtered)
        return [SearchResult(r.content, r.metadata, r.score) for r in deduped]
```

> `metadata.source_id`가 없으면 중복 제거가 동작하지 않으므로 저장 단계에서 필수로 넣어야 합니다.
> 예시는 개념 설명을 위한 축약 코드입니다. import/예외 처리/설정 주입 등은 생략되어 있습니다.

---

## 3. LangChain 기준 구현 흐름(권장)

LangChain을 기준으로 하면 **청크 → 임베딩 → 저장 → 검색** 흐름을 간단하게 구성할 수 있습니다.

```python
"""
목적: LangChain 구성요소로 벡터 검색 파이프라인을 구축한다.
설명: TextSplitter → Embeddings → PGVectorStore → Retriever 순서로 구성한다.
디자인 패턴: Facade
"""

from langchain_core.documents import Document
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_postgres import PGEngine, PGVectorStore


CONNECTION_STRING = (
    "postgresql+psycopg://langchain:langchain@localhost:5432/langchain"
)
TABLE_NAME = "documents"
VECTOR_SIZE = 1536

engine = PGEngine.from_connection_string(url=CONNECTION_STRING)
engine.init_vectorstore_table(
    table_name=TABLE_NAME,
    vector_size=VECTOR_SIZE,
)

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=120,
)

docs = [
    Document(page_content="환불 정책 안내 문서 ...", metadata={"category": "policy"}),
    Document(page_content="결제 실패 대응 가이드 ...", metadata={"category": "guide"}),
]
chunked_docs = splitter.split_documents(docs)

embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
store = PGVectorStore.create_sync(
    engine=engine,
    table_name=TABLE_NAME,
    embedding_service=embeddings,
)
store.add_documents(chunked_docs)

retriever = store.as_retriever(search_kwargs={"k": 5})
results = retriever.invoke("환불 절차를 알려줘")
```

**핵심 포인트**

- 청크 분할은 `TextSplitter`로 표준화하는 것이 안전함
- 검색은 **리트리버(retriever)** 인터페이스로 추상화하면 다른 검색 방식으로 교체가 쉬움
- `search_kwargs`에 `k`를 명시해 결과 수를 통제해야 함

---

## 4. 점수 스케일과 임계값 튜닝

벡터 검색 결과의 점수는 **모델, 연산자, 구현 방식**에 따라 스케일이 다릅니다.
즉, “점수 0.4면 충분히 좋다” 같은 일반 규칙은 없습니다.
반드시 **자체 데이터 기준으로 임계값을 캘리브레이션**해야 합니다.

### 1) 점수 스케일이 달라지는 이유

- 코사인 거리(<=>)는 **0에 가까울수록 유사**
- 내적/유사도 점수는 **클수록 유사**
- 라이브러리마다 “점수” 정의가 다를 수 있음

**정리**

- “거리 기반”이면 **작을수록 좋다**
- “유사도 기반”이면 **클수록 좋다**
- 팀 내에서 **점수 해석 기준을 먼저 문서화**해야 한다

### 2) 기본 튜닝 절차(추천)

1) 평가용 질문 30~100개를 준비한다
2) 각 질문에 대해 **Top-K 결과**를 수집한다
3) 사람이 “정답/오답”을 라벨링한다
4) 점수 분포를 보고 **임계값 후보**를 고른다
5) 누락(Recall)과 오탐(Precision)의 균형을 맞춘다

### 3) 최소 임계값 + Top-K 조합 전략

- `Top-K`를 늘리면 누락은 줄고 비용은 증가한다
- `min_score`를 올리면 정확도는 늘고 누락이 증가한다
- 실무에서는 **Top-K를 먼저 안정화**한 뒤 임계값을 조정한다

### 4) 거리 점수를 유사도로 바꾸는 방법(선택)

거리 기반 점수를 그대로 쓰기 어렵다면, 간단히 **역수 변환**으로 유사도를 만들 수 있습니다.

```python
"""
목적: 거리 기반 점수를 유사도 스케일로 변환한다.
설명: 1 / (1 + distance) 방식으로 단조 변환한다.
디자인 패턴: Strategy
"""

def distance_to_similarity(distance: float) -> float:
    """거리를 유사도 점수로 변환한다."""
    return 1.0 / (1.0 + distance)
```

### 5) 점수 정규화의 필요성

검색 전략이 여러 개라면 점수 스케일이 다릅니다.
이 경우 **정규화(0~1 스케일링)** 없이 병합하면 특정 전략만 편중됩니다.

```python
"""
목적: 점수 정규화를 통해 다양한 검색 결과를 공정하게 비교한다.
설명: 최소/최대 스케일링으로 점수를 0~1로 보정한다.
디자인 패턴: Strategy
"""

def min_max_scale(scores: list[float]) -> list[float]:
    """점수를 0~1 범위로 정규화한다."""
    if not scores:
        return []
    min_v, max_v = min(scores), max(scores)
    if min_v == max_v:
        return [1.0 for _ in scores]
    return [(s - min_v) / (max_v - min_v) for s in scores]
```

### 6) 임계값을 서비스에 반영하는 방식

- **고정 임계값**: 운영이 단순하지만 도메인 변화에 취약
- **도메인별 임계값**: 카테고리/문서 유형별로 다르게 설정
- **동적 임계값**: 최근 검색 성능에 따라 자동 조정

### 7) 실무에서 자주 쓰는 기준

- **퍼센타일 기준**: 상위 10~20% 점수만 채택
- **절대 컷 기준**: 학습한 임계값을 고정해 사용
- **혼합 기준**: Top-K 후 임계값 필터를 한 번 더 적용

---

## 5. 서비스에서 필요한 주요 요소(운영 관점)

서비스에서 벡터 검색을 운영하려면 아래 요소가 필요합니다.

### 1) 데이터 파이프라인

- 문서 수집 → 정제 → 청크 → 임베딩 → 저장
- 배치 처리/재시도/버전 관리 포함

### 2) 검색 API

- 사용자 질의 → 임베딩 → DB 검색 → 후처리 → 응답
- 캐시, 타임아웃, 로깅 포함

### 3) 품질 관리

- 검색 결과 품질 지표(정확도/누락/중복)
- 임계값 튜닝 전략

### 4) 운영 모니터링

- 검색 지연 시간, 실패율, 비용 모니터링
- 인덱스/테이블 건강 상태 점검

### 5) 보안/권한 관리

- 메타데이터 필터 기반 권한 제어
- 내부 문서 노출 방지

---

## 6. 설계 시 반드시 고려해야 할 질문

- 쿼리 임베딩은 어떤 모델을 쓰는가?
- 임베딩 차원과 인덱스가 정확히 맞는가?
- 메타데이터 필터링 기준은 안정적인가?
- 청크 크기/오버랩을 문서화했는가?
- 검색 결과의 임계값은 어떻게 결정하는가?
- 지표는 무엇을 기준으로 개선할 것인가?

---

## 7. 흔한 실수

- 청크 분할 기준이 불명확해 결과가 흔들림
- 점수 스케일을 이해하지 못해 임계값이 잘못됨
- 인덱스 없이 대용량을 운영해 속도가 급격히 저하
- 메타데이터 필터가 느려져 검색 지연 증가

---

## 8. 체크리스트

- 검색 파이프라인 흐름을 문서화했는가?
- 각 단계의 책임 클래스가 명확한가?
- 운영 지표를 정의했는가?
- 권한 필터가 적용되는가?
- 품질 개선 기준이 있는가?
